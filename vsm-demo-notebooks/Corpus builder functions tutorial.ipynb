{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Corpus Builder Functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`vsm.corpus.util.corpusbuilders` module provides convenience functions for making `Corpus` object. A `Corpus` object can be generated from a *list*, *file*, *directory* or *collections of books*. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from vsm.corpus.util.corpusbuilders import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Building simple `Corpus` with sample data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following functions make `Corpus` object with little or no data. Words in the `Corpus` could be str or int type.\n",
      "\n",
      "<ul>\n",
      "    <li>random_corpus</li>\n",
      "    <li>toy_corpus</li>\n",
      "    <li>corpus_fromlist</li>\n",
      "</ul>\n",
      "\n",
      "**Note**: let's see if this inline style works properly with Sphinx. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **`random_corpus` generates a `Corpus` made of random integers, where words in the `Corpus` are integers converted to strings.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = random_corpus(500, 50, 1, 20, context_type='sentence')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Corpus object above has a corpus size of 500, 50 words, and sentences whose lengths range from 1 to 20. The context type, 'sentence', is treated as the context (document) for modeling purposes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c.view_contexts('sentence')[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "[array([  4, 131, 497, 265, 499], dtype=int32),\n",
        " array([351, 148, 223,  16, 232, 257, 156, 282, 371, 490, 203, 176, 427,\n",
        "        99,  14, 190], dtype=int32),\n",
        " array([251, 374, 279, 373, 452, 354, 364], dtype=int32),\n",
        " array([250], dtype=int32),\n",
        " array([393, 409, 373, 434, 355, 255, 408, 429], dtype=int32)]"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`Corpus.view_contexts(context_type)` returns a list of arrays where each array is a context tokenization. The first context consists of 5 \"words\", the second context 16 \"words\".\n",
      "\n",
      "By default, entries in the array are shown as integer representations of the words. When `as_strings` parameter is set to `True`, the \"words\" from the corpus are shown as strings."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c.view_contexts('sentence', as_strings=True)[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "[array(['101', '216', '97', '337', '99'], \n",
        "      dtype='|S8'),\n",
        " array(['414', '231', '3', '112', '307', '33', '239', '352', '432', '90',\n",
        "       '281', '257', '483', '188', '110', '27'], \n",
        "      dtype='|S8'),\n",
        " array(['324', '435', '35', '434', '56', '417', '426'], \n",
        "      dtype='|S8'),\n",
        " array(['323'], \n",
        "      dtype='|S8'),\n",
        " array(['452', '467', '434', '49', '418', '328', '466', '485'], \n",
        "      dtype='|S8')]"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **`toy_corpus` builds a `Corpus` with text given as a string or a plain text file.**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`toy_corpus` can take text either as a `.txt` file or string of text. The parameter `is_filename` is `False` by default, so when using a `.txt` file `is_filename` should be set to `True`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = toy_corpus('lesmiserables_preface.txt', is_filename=True, nltk_stop=True, add_stop=[])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Removing stop words\n",
        "Rebuilding corpus\n",
        "Recomputing token breaks: document\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c.view_contexts('document', as_strings=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[array(['long', 'shall', 'exist', 'virtue', 'law', 'custom', 'decrees',\n",
        "       'damnation', 'pronounced', 'society', 'artificially', 'creating',\n",
        "       'hells', 'amid', 'civilization', 'earth', 'adding', 'element',\n",
        "       'human', 'fate', 'divine', 'destiny'], \n",
        "      dtype='|S12'),\n",
        " array(['long', 'three', 'great', 'problems', 'century', 'degradation',\n",
        "       'man', 'pauperism', 'corruption', 'woman', 'hunger', 'crippling',\n",
        "       'children', 'lack', 'light', 'unsolved'], \n",
        "      dtype='|S12'),\n",
        " array(['long', 'social', 'asphyxia', 'possible', 'part', 'world'], \n",
        "      dtype='|S12'),\n",
        " array(['words', 'still', 'wider', 'significance', 'long', 'ignorance',\n",
        "       'poverty', 'exist', 'earth', 'books', 'nature', 'les', 'miserables',\n",
        "       'fail', 'use'], \n",
        "      dtype='|S12')]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each array shows the context 'document', and many are not readable because words in English nltk stoplist are filtered by `nltk_stop=True`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = 'So long as there shall exist, by virtue of law and custom, decrees of\\\n",
      "        damnation pronounced by society, artificially creating hells amid the civilization\\\n",
      "        of earth, and adding the element of human fate to divine destiny;\\n\\n\\\n",
      "        so long as the three great problems of the century--the degradation of man through pauperism,\\\n",
      "        the corruption of woman through hunger, the crippling of children through lack of light--are unsolved;\\n\\n\\\n",
      "        so long as social asphyxia is possible in any part of the world;\\n\\n\\\n",
      "        --in other words, and with a still wider significance, so long as ignorance\\\n",
      "        and poverty exist on earth, books of the nature of Les Miserables cannot\\\n",
      "        fail to be of use.\\n\\n'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`text` defined above is the same data that is in 'lesmiserables_preface.txt'. As shown below, `toy_corpus` returns a `Corpus` object with same contexts whether the data is given as a file or a string or text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = toy_corpus(text, is_filename=False, nltk_stop=True, add_stop=[])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Removing stop words\n",
        "Rebuilding corpus\n",
        "Recomputing token breaks: document\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c.view_contexts('document', as_strings=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[array(['long', 'shall', 'exist', 'virtue', 'law', 'custom', 'decrees',\n",
        "       'damnation', 'pronounced', 'society', 'artificially', 'creating',\n",
        "       'hells', 'amid', 'civilization', 'earth', 'adding', 'element',\n",
        "       'human', 'fate', 'divine', 'destiny'], \n",
        "      dtype='|S12'),\n",
        " array(['long', 'three', 'great', 'problems', 'century', 'degradation',\n",
        "       'man', 'pauperism', 'corruption', 'woman', 'hunger', 'crippling',\n",
        "       'children', 'lack', 'light', 'unsolved'], \n",
        "      dtype='|S12'),\n",
        " array(['long', 'social', 'asphyxia', 'possible', 'part', 'world'], \n",
        "      dtype='|S12'),\n",
        " array(['words', 'still', 'wider', 'significance', 'long', 'ignorance',\n",
        "       'poverty', 'exist', 'earth', 'books', 'nature', 'les', 'miserables',\n",
        "       'fail', 'use'], \n",
        "      dtype='|S12')]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **`corpus_fromlist` builds a `Corpus` from a list of lists.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ls = [['a', 'b'], ['c'], ['d', 'e']]\n",
      "c = corpus_fromlist(ls, context_type='sentence')\n",
      "\n",
      "c.view_metadata('sentence')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "array([(2, 'sentence_0'), (3, 'sentence_1'), (5, 'sentence_2')], \n",
        "      dtype=[('idx', '<i8'), ('sentence_label', '|S10')])"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If your text data is in a list format, this function may be useful. The sublists are treated as contexts.\n",
      "\n",
      "`Corpus.view_metadata(context)` shows metadata as a structured numpy array. In the example, index information, 'idx' indicates where the context is tokenized in the entire corpus. The last 'idx' entry equals to the length of the textual corpus. 'sentence_label' is index of sublists in the given list as strings."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Building `Corpus` from a file or directory"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`Corpus` can also be generated from \n",
      "<ul>\n",
      "    <li>file_corpus : a plain text file</li>\n",
      "    <li>dir_corpus : a directory containing plain text files</li>\n",
      "    <li>coll_corpus : a collection of books</li>\n",
      "</ul>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **`file_corpus` builds a `Corpus` with a plain text file. **"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = file_corpus('lesmiserables_preface.txt', nltk_stop=True, stop_freq=0, add_stop=None)\n",
      "\n",
      "c.save('lesmis_pf_nltk_freq3.npz')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Removing stop words\n",
        "Rebuilding corpus\n",
        "Recomputing token breaks: paragraph\n",
        "Recomputing token breaks: sentence\n",
        "Saving corpus as lesmis_pf_nltk_freq3.npz\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`file_corpus` takes a plain text file and optional parameters that filters words in the textual corpus. If `nltk_stop` is set to `True`, words in English nltk stoplist are eliminated. `stop_freq` filters words in corpus by the number of frequencies. If a word appears less than or equal to `stop_freq`, then the word is eliminated to optimize analysis from Natural Semantic models.\n",
      "\n",
      "For later use, `Corpus` object can be saved in an `.npz` file with `Corpus.save(filename)`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c.view_metadata('paragraph')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "array([(22, '0'), (38, '1'), (44, '2'), (59, '3')], \n",
        "      dtype=[('idx', '<i4'), ('paragraph_label', '|S8')])"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "metadata of `c` includes index and tokenization indices. 'idx' indicates where the context is tokenized in the entire corpus. 'paragraph_label' has index of paragraphs as strings."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c.view_contexts('paragraph', as_strings=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "[array(['long', 'shall', 'exist', 'virtue', 'law', 'custom', 'decrees',\n",
        "       'damnation', 'pronounced', 'society', 'artificially', 'creating',\n",
        "       'hells', 'amid', 'civilization', 'earth', 'adding', 'element',\n",
        "       'human', 'fate', 'divine', 'destiny'], \n",
        "      dtype='|S12'),\n",
        " array(['long', 'three', 'great', 'problems', 'century', 'degradation',\n",
        "       'man', 'pauperism', 'corruption', 'woman', 'hunger', 'crippling',\n",
        "       'children', 'lack', 'light', 'unsolved'], \n",
        "      dtype='|S12'),\n",
        " array(['long', 'social', 'asphyxia', 'possible', 'part', 'world'], \n",
        "      dtype='|S12'),\n",
        " array(['words', 'still', 'wider', 'significance', 'long', 'ignorance',\n",
        "       'poverty', 'exist', 'earth', 'books', 'nature', 'les', 'miserables',\n",
        "       'fail', 'use'], \n",
        "      dtype='|S12')]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **`dir_corpus` builds a `Corpus` from a directory containing plain text files. ** "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = dir_corpus('lesmiserables', chunk_name='chapter', stop_freq=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tokenizing CHAPTER I.txt\n",
        "Tokenizing"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " CHAPTER II.txt\n",
        "Tokenizing"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " CHAPTER III.txt\n",
        "Tokenizing CHAPTER IV.txt\n",
        "Tokenizing"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " CHAPTER V.txt\n",
        "Tokenizing CHAPTER VI.txt\n",
        "Tokenizing"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " CHAPTER VII.txt\n",
        "Computing collection frequencies"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Selecting words of frequency <="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Removing stop words\n",
        "Rebuilding corpus\n",
        "Recomputing token breaks: chapter\n",
        "Recomputing token breaks: paragraph\n",
        "Recomputing token breaks: sentence\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`dir_corpus` retains the file-level tokenization. `chunk_name` is the name that represents the plain text file. In this case, a 'chapter' is stored in each plain text files. For example, if the directory was a journal with articles, the text files (the chunks) could be named 'article'."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c.view_metadata('chapter')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "array([(216, 'CHAPTER I.txt'), (635, 'CHAPTER II.txt'),\n",
        "       (778, 'CHAPTER III.txt'), (1287, 'CHAPTER IV.txt'),\n",
        "       (1501, 'CHAPTER V.txt'), (1978, 'CHAPTER VI.txt'),\n",
        "       (2245, 'CHAPTER VII.txt')], \n",
        "      dtype=[('idx', '<i4'), ('chapter_label', '|S15')])"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **`coll_corpus` builds a `Corpus` from a directory of directories where a directory contains plain text files.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = coll_corpus('coll_lesmis', nltk_stop=True, stop_freq=3, add_stop=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tokenizing book1\n",
        "Tokenizing"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " book2\n",
        "Computing collection frequencies"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Selecting words of frequency <="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Removing stop words\n",
        "Rebuilding corpus\n",
        "Recomputing token breaks: book\n",
        "Recomputing token breaks:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " page\n",
        "Recomputing token breaks: sentence\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`coll_corpus` generates metadata which includes book, page, sentence labels as well as file information. <br>\n",
      "'book1' and 'book2' are subdirectories in the directory 'coll_lesmis'. Files 'book1/b1_c1.txt', 'book2/b2_c9.txt', etc. are plain text files in the subdirectories."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c.view_metadata('page')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "array([(346, '0', 'book1', 'book1/b1_c1.txt'),\n",
        "       (1809, '1', 'book1', 'book1/b1_c10.txt'),\n",
        "       (2308, '2', 'book1', 'book1/b1_c11.txt'),\n",
        "       (2609, '3', 'book1', 'book1/b1_c12.txt'),\n",
        "       (3097, '4', 'book1', 'book1/b1_c13.txt'),\n",
        "       (3361, '5', 'book1', 'book1/b1_c14.txt'),\n",
        "       (3943, '6', 'book1', 'book1/b1_c2.txt'),\n",
        "       (4154, '7', 'book1', 'book1/b1_c3.txt'),\n",
        "       (5019, '8', 'book1', 'book1/b1_c4.txt'),\n",
        "       (5350, '9', 'book1', 'book1/b1_c5.txt'),\n",
        "       (6080, '10', 'book1', 'book1/b1_c6.txt'),\n",
        "       (6499, '11', 'book1', 'book1/b1_c7.txt'),\n",
        "       (6906, '12', 'book1', 'book1/b1_c8.txt'),\n",
        "       (7368, '13', 'book1', 'book1/b1_c9.txt'),\n",
        "       (8857, '14', 'book2', 'book2/b2_c1.txt'),\n",
        "       (9235, '15', 'book2', 'book2/b2_c10.txt'),\n",
        "       (9732, '16', 'book2', 'book2/b2_c11.txt'),\n",
        "       (10188, '17', 'book2', 'book2/b2_c12.txt'),\n",
        "       (11378, '18', 'book2', 'book2/b2_c13.txt'),\n",
        "       (11874, '19', 'book2', 'book2/b2_c2.txt'),\n",
        "       (12649, '20', 'book2', 'book2/b2_c3.txt'),\n",
        "       (13082, '21', 'book2', 'book2/b2_c4.txt'),\n",
        "       (13276, '22', 'book2', 'book2/b2_c5.txt'),\n",
        "       (13985, '23', 'book2', 'book2/b2_c6.txt'),\n",
        "       (14925, '24', 'book2', 'book2/b2_c7.txt'),\n",
        "       (15114, '25', 'book2', 'book2/b2_c8.txt'),\n",
        "       (15259, '26', 'book2', 'book2/b2_c9.txt')], \n",
        "      dtype=[('idx', '<i4'), ('page_label', '|S8'), ('book_label', '|S5'), ('file', '|S16')])"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you want to add more metadata about the corpus, here is a method for that. **`add_metadata(corp, context_type, new_field_name, metadata)`** returns a corpus with new metadata added labeled as a new_field_name."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from vsm.corpus.util import add_metadata \n",
      "\n",
      "metadata = ['M. Myriel', 'M. Myriel Becomes M. Welcome', 'A Hard Bishoric For A Good Bishop',\n",
      "            'Works Corresponding To Words', 'Monseigneur Bienvenu Made His Cassocks Last Too Long',\n",
      "            'Who Guarded His House For Him', 'Cravatte', 'Philosophy After Drinking',\n",
      "            'The Brother As Depicted By The Sister', 'The Bishop In The Presence Of An Unknown Light',\n",
      "            'A Restriction', 'The Solitude Of Monseigneur Welcome', 'What He Believed', 'What He Thought',\n",
      "            'The Evening Of A Day Of Walking', 'Prudence Counselled To Wisdom', \n",
      "            'The Heroism Of Passive Obedience', 'Details Concertning The Cheese-Dairies OF Pontarlier',\n",
      "            'Tranquility', 'Jean Valjean', 'The Interior Of Despair', 'Billows And Shadows', 'New Troubles',\n",
      "            'The Man Aroused', 'What He Does', 'The Bishop Works', 'Little Gervais']\n",
      "\n",
      "corp = add_metadata(c, 'page', 'chapter_name', metadata)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's say we want to include the name of the chapter in the metadata. We obtain the metadata as a list with same number of elements that match up with the `context_type` metadata."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corp.view_metadata('page')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "array([(346, '0', 'book1', 'book1/b1_c1.txt', 'M. Myriel'),\n",
        "       (1809, '1', 'book1', 'book1/b1_c10.txt', 'M. Myriel Becomes M. Welcome'),\n",
        "       (2308, '2', 'book1', 'book1/b1_c11.txt', 'A Hard Bishoric For A Good Bishop'),\n",
        "       (2609, '3', 'book1', 'book1/b1_c12.txt', 'Works Corresponding To Words'),\n",
        "       (3097, '4', 'book1', 'book1/b1_c13.txt', 'Monseigneur Bienvenu Made His Cassocks Last Too Long'),\n",
        "       (3361, '5', 'book1', 'book1/b1_c14.txt', 'Who Guarded His House For Him'),\n",
        "       (3943, '6', 'book1', 'book1/b1_c2.txt', 'Cravatte'),\n",
        "       (4154, '7', 'book1', 'book1/b1_c3.txt', 'Philosophy After Drinking'),\n",
        "       (5019, '8', 'book1', 'book1/b1_c4.txt', 'The Brother As Depicted By The Sister'),\n",
        "       (5350, '9', 'book1', 'book1/b1_c5.txt', 'The Bishop In The Presence Of An Unknown Light'),\n",
        "       (6080, '10', 'book1', 'book1/b1_c6.txt', 'A Restriction'),\n",
        "       (6499, '11', 'book1', 'book1/b1_c7.txt', 'The Solitude Of Monseigneur Welcome'),\n",
        "       (6906, '12', 'book1', 'book1/b1_c8.txt', 'What He Believed'),\n",
        "       (7368, '13', 'book1', 'book1/b1_c9.txt', 'What He Thought'),\n",
        "       (8857, '14', 'book2', 'book2/b2_c1.txt', 'The Evening Of A Day Of Walking'),\n",
        "       (9235, '15', 'book2', 'book2/b2_c10.txt', 'Prudence Counselled To Wisdom'),\n",
        "       (9732, '16', 'book2', 'book2/b2_c11.txt', 'The Heroism Of Passive Obedience'),\n",
        "       (10188, '17', 'book2', 'book2/b2_c12.txt', 'Details Concertning The Cheese-Dairies OF Pontarlier'),\n",
        "       (11378, '18', 'book2', 'book2/b2_c13.txt', 'Tranquility'),\n",
        "       (11874, '19', 'book2', 'book2/b2_c2.txt', 'Jean Valjean'),\n",
        "       (12649, '20', 'book2', 'book2/b2_c3.txt', 'The Interior Of Despair'),\n",
        "       (13082, '21', 'book2', 'book2/b2_c4.txt', 'Billows And Shadows'),\n",
        "       (13276, '22', 'book2', 'book2/b2_c5.txt', 'New Troubles'),\n",
        "       (13985, '23', 'book2', 'book2/b2_c6.txt', 'The Man Aroused'),\n",
        "       (14925, '24', 'book2', 'book2/b2_c7.txt', 'What He Does'),\n",
        "       (15114, '25', 'book2', 'book2/b2_c8.txt', 'The Bishop Works'),\n",
        "       (15259, '26', 'book2', 'book2/b2_c9.txt', 'Little Gervais')], \n",
        "      dtype=[('idx', '<i4'), ('page_label', '|S8'), ('book_label', '|S5'), ('file', '|S16'), ('chapter_name', '|S52')])"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All of these generated `Corpus` objects could be saved with `Corpus.save(filename)` for later use."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}